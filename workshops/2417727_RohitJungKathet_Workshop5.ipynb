{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing train test split function\n",
    "def train_test_split(\n",
    "    X: np.ndarray, y: np.ndarray, test_size: float = 0.3, random_seed: int = 42\n",
    "):\n",
    "    \"\"\"Separates the dataset into train and test sets\n",
    "    Args:\n",
    "        X (np.ndarray): Feature matrix\n",
    "        y (np.ndarray): Label Vector\n",
    "        test_size (float, optional):\n",
    "          Proportion of dataset to include in the test split\n",
    "          Defaults to 0.3.\n",
    "        random_seed (int, optional):\n",
    "          Seed for reproduceability\n",
    "          Defaults to 41.\n",
    "    Returns:\n",
    "      X_train, X_test, y_train, y_test : np.ndarray\n",
    "        Training and testing splits of features and target.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    test_split_size = int(len(X) * test_size)\n",
    "    test_indices = indices[:test_split_size]\n",
    "    train_indices = indices[test_split_size:]\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "def cost_function(X, Y, W):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "      This function finds the Mean Square Error.\n",
    "    Input parameters:\n",
    "      X: Feature Matrix\n",
    "      Y: Target Matrix\n",
    "      W: Weight Matrix\n",
    "    Output Parameters:\n",
    "      cost: accumulated mean square error.\n",
    "    \"\"\"\n",
    "    m = len(Y)\n",
    "    predictions = X.dot(W)\n",
    "    errors = Y - predictions\n",
    "    squared_errors = errors**2\n",
    "    cost = np.mean(squared_errors)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceed Further\n"
     ]
    }
   ],
   "source": [
    "# Test case\n",
    "X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "Y_test = np.array([3, 7, 11])\n",
    "W_test = np.array([1, 1])\n",
    "cost = cost_function(X_test, Y_test, W_test)\n",
    "if cost == 0:\n",
    "  print(\"Proceed Further\")\n",
    "else:\n",
    "  print(\"something went wrong: Reimplement a cost function\")\n",
    "  print(\"Cost function output:\", cost_function(X_test, Y_test, W_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, W, alpha, iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
    "    Parameters:\n",
    "      X (numpy.ndarray): Feature matrix (m x n).\n",
    "      Y (numpy.ndarray): Target vector (m x 1).\n",
    "      W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
    "      alpha (float): Learning rate.\n",
    "      iterations (int): Number of iterations for gradient descent.\n",
    "    Returns:\n",
    "      tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values.\n",
    "      W_update (numpy.ndarray): Updated parameters (n x 1).\n",
    "      cost_history (list): History of cost values over iterations.\n",
    "    \"\"\"\n",
    "    # Initialize cost history\n",
    "    cost_history = [0] * iterations\n",
    "    W_START = 0\n",
    "    # Number of samples\n",
    "    m = len(Y)\n",
    "    for iteration in range(iterations):\n",
    "        # Step 1: Hypothesis Values\n",
    "        Y_pred = X.dot(W)\n",
    "        # Step 2: Difference between Hypothesis and Actual Y\n",
    "        loss = Y_pred - Y\n",
    "        # Step 3: Gradient Calculation\n",
    "        dw = (2 / m) * X.T.dot(loss)\n",
    "        # Step 4: Updating Values of W using Gradient\n",
    "        W_update = W_START - alpha * dw\n",
    "        # Step 5: New Cost Value\n",
    "        cost = cost_function(X, Y, W_update)\n",
    "        cost_history[iteration] = cost\n",
    "    return W_update, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Parameters: [-0.00321979 -0.00367639 -0.0026994 ]\n",
      "Cost History: [0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753]\n"
     ]
    }
   ],
   "source": [
    "# Generate random test data\n",
    "np.random.seed(0) # For reproducibility\n",
    "X = np.random.rand(100, 3) # 100 samples, 3 features\n",
    "Y = np.random.rand(100)\n",
    "W = np.random.rand(3) # Initial guess for parameters\n",
    "# Set hyperparameters\n",
    "alpha = 0.01\n",
    "iterations = 1000\n",
    "# Test the gradient_descent function\n",
    "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
    "# Print the final parameters and cost history\n",
    "print(\"Final Parameters:\", final_params)\n",
    "print(\"Cost History:\", cost_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation - RMSE\n",
    "def rmse(Y, Y_pred):\n",
    "    \"\"\"\n",
    "    This Function calculates the Root Mean Squres.\n",
    "    Input Arguments:\n",
    "      Y: Array of actual(Target) Dependent Varaibles.\n",
    "      Y_pred: Array of predeicted Dependent Varaibles.\n",
    "    Output Arguments:\n",
    "      rmse: Root Mean Square.\n",
    "    \"\"\"\n",
    "    errors = Y - Y_pred\n",
    "    squared_errors = errors**2\n",
    "    mse = np.mean(squared_errors)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation - R2\n",
    "def r2(Y, Y_pred):\n",
    "    \"\"\"\n",
    "    This Function calculates the R Squared Error.\n",
    "    Input Arguments:\n",
    "      Y: Array of actual(Target) Dependent Varaibles.\n",
    "      Y_pred: Array of predeicted Dependent Varaibles.\n",
    "    Output Arguments:\n",
    "      rsquared: R Squared Error.\n",
    "    \"\"\"\n",
    "    mean_y = np.mean(Y)\n",
    "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
    "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/student.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR-Squared on Test Set:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_r2)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 35\u001b[0m   main()\n",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m      3\u001b[0m   \u001b[38;5;66;03m# Step 1: Load the dataset\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m   data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../datasets/student.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m   \u001b[38;5;66;03m# Step 2: Split the data into features (X) and target (Y)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m   X \u001b[38;5;241m=\u001b[39m data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMath\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReading\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;66;03m# Features: Math and Reading marks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/student.csv'"
     ]
    }
   ],
   "source": [
    "# Main Function\n",
    "def main():\n",
    "  # Step 1: Load the dataset\n",
    "  data = pd.read_csv('../datasets/student.csv')\n",
    "\n",
    "  # Step 2: Split the data into features (X) and target (Y)\n",
    "  X = data[['Math', 'Reading']].values # Features: Math and Reading marks\n",
    "  Y = data['Writing'].values # Target: Writing marks\n",
    "\n",
    "  # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
    "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_seed=42)\n",
    "\n",
    "  # Step 4: Initialize weights (W) to zeros, learning rate and number of iterations\n",
    "  W = np.zeros(X_train.shape[1]) # Initialize weights\n",
    "  alpha = 0.00001 # Learning rate\n",
    "  iterations = 1000 # Number of iterations for gradient descent\n",
    "\n",
    "  # Step 5: Perform Gradient Descent\n",
    "  W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
    "\n",
    "  # Step 6: Make predictions on the test set\n",
    "  Y_pred = np.dot(X_test, W_optimal)\n",
    "\n",
    "  # Step 7: Evaluate the model using RMSE and R-Squared\n",
    "  model_rmse = rmse(Y_test, Y_pred)\n",
    "  model_r2 = r2(Y_test, Y_pred)\n",
    "\n",
    "  # Step 8: Output the results\n",
    "  print(\"Final Weights:\", W_optimal)\n",
    "  print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
    "  print(\"RMSE on Test Set:\", model_rmse)\n",
    "  print(\"R-Squared on Test Set:\", model_r2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
