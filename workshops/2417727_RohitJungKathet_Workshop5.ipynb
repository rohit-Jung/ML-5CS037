{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing train test split function\n",
    "def train_test_split(\n",
    "    X: np.ndarray, y: np.ndarray, test_size: float = 0.3, random_seed: int = 42\n",
    "):\n",
    "    \"\"\"Separates the dataset into train and test sets\n",
    "    Args:\n",
    "        X (np.ndarray): Feature matrix\n",
    "        y (np.ndarray): Label Vector\n",
    "        test_size (float, optional):\n",
    "          Proportion of dataset to include in the test split\n",
    "          Defaults to 0.3.\n",
    "        random_seed (int, optional):\n",
    "          Seed for reproduceability\n",
    "          Defaults to 41.\n",
    "    Returns:\n",
    "      X_train, X_test, y_train, y_test : np.ndarray\n",
    "        Training and testing splits of features and target.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    test_split_size = int(len(X) * test_size)\n",
    "    test_indices = indices[:test_split_size]\n",
    "    train_indices = indices[test_split_size:]\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "def cost_function(X, Y, W):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "      This function finds the Mean Square Error.\n",
    "    Input parameters:\n",
    "      X: Feature Matrix\n",
    "      Y: Target Matrix\n",
    "      W: Weight Matrix\n",
    "    Output Parameters:\n",
    "      cost: accumulated mean square error.\n",
    "    \"\"\"\n",
    "    m = len(Y)\n",
    "    predictions = X.dot(W)\n",
    "    errors = Y - predictions\n",
    "    squared_errors = errors**2\n",
    "    cost = np.mean(squared_errors)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceed Further\n"
     ]
    }
   ],
   "source": [
    "# Test case\n",
    "X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "Y_test = np.array([3, 7, 11])\n",
    "W_test = np.array([1, 1])\n",
    "cost = cost_function(X_test, Y_test, W_test)\n",
    "if cost == 0:\n",
    "  print(\"Proceed Further\")\n",
    "else:\n",
    "  print(\"something went wrong: Reimplement a cost function\")\n",
    "  print(\"Cost function output:\", cost_function(X_test, Y_test, W_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, W, alpha, iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
    "    Parameters:\n",
    "      X (numpy.ndarray): Feature matrix (m x n).\n",
    "      Y (numpy.ndarray): Target vector (m x 1).\n",
    "      W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
    "      alpha (float): Learning rate.\n",
    "      iterations (int): Number of iterations for gradient descent.\n",
    "    Returns:\n",
    "      tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values.\n",
    "      W (numpy.ndarray): Updated parameters (n x 1).\n",
    "      cost_history (list): History of cost values over iterations.\n",
    "    \"\"\"\n",
    "    # Initialize cost history\n",
    "    cost_history = [0] * iterations\n",
    "    # Number of samples\n",
    "    m = len(Y)\n",
    "    for iteration in range(iterations):\n",
    "        # Step 1: Hypothesis Values\n",
    "        Y_pred = np.dot(X, W)\n",
    "        # Step 2: Difference between Hypothesis and Actual Y\n",
    "        loss = Y_pred - Y\n",
    "        # Step 3: Gradient Calculation\n",
    "        dw = (1 / m) * np.dot(X.T, loss)\n",
    "        # Step 4: Updating Values of W using Gradient\n",
    "        W -= alpha * dw\n",
    "        # Step 5: New Cost Value\n",
    "        cost = cost_function(X, Y, W)\n",
    "        cost_history[iteration] = cost\n",
    "    return W, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Parameters: [0.20551667 0.54295081 0.10388027]\n",
      "Cost History: [0.21422394189320307, 0.21269761199879803, 0.21119652631361233, 0.20972025896641117, 0.2082683912857068, 0.2068405116780125, 0.2054362155081552, 0.2040551049816124, 0.20269678902883864, 0.2013608831915474, 0.2000470095109174, 0.19875479641768756, 0.1974838786241122, 0.19623389701774197, 0.1950044985570019, 0.1937953361685344, 0.19260606864627905, 0.1914363605522583, 0.19028588211904132, 0.18915430915385684, 0.18804132294432793, 0.1869466101658003, 0.18586986279023826, 0.18481077799666035, 0.18376905808309085, 0.182744410379999, 0.18173654716520246, 0.18074518558021005, 0.17977004754797835, 0.17881085969206015, 0.17786735325711905, 0.17693926403078866, 0.17602633226685335, 0.17512830260972778, 0.17424492402021324, 0.17337594970251013, 0.17252113703246416, 0.1716802474870256, 0.17085304657490266, 0.17003930376838602, 0.1692387924363272, 0.16845128977824977, 0.16767657675957526, 0.1669144380479437, 0.16616466195061164, 0.1654270403529085, 0.16470136865773358, 0.16398744572607632, 0.16328507381854224, 0.16259405853786774, 0.16191420877240706, 0.16124533664057478, 0.16058725743622784, 0.15993978957497107, 0.159302754541371, 0.15867597683706175, 0.15805928392972918, 0.1574525062029569, 0.1568554769069211, 0.15626803210991874, 0.1556900106507156, 0.15512125409169983, 0.1545616066728281, 0.15401091526635027, 0.15346902933229978, 0.15293580087473624, 0.15241108439872897, 0.15189473686806687, 0.15138661766368414, 0.15088658854278858, 0.15039451359868145, 0.1499102592212564, 0.14943369405816656, 0.14896468897664825, 0.1485031170259904, 0.14804885340063825, 0.14760177540392141, 0.147161762412395, 0.1467286958407838, 0.1463024591075192, 0.14588293760085935, 0.14547001864558134, 0.14506359147023742, 0.14466354717496466, 0.1442697786998386, 0.14388218079376278, 0.14350064998388365, 0.14312508454552297, 0.1427553844726187, 0.14239145144866572, 0.14203318881814767, 0.14168050155845247, 0.141333296252262, 0.14099148106040926, 0.1406549656951943, 0.14032366139415145, 0.13999748089425984, 0.13967633840659047, 0.13936014959138188, 0.13904883153353687, 0.1387423027185343, 0.1384404830087475, 0.1381432936201637, 0.13785065709949668, 0.137562497301687, 0.13727873936778193, 0.13699930970319016, 0.1367241359563039, 0.13645314699748248, 0.13618627289839122, 0.13592344491169, 0.13566459545106507, 0.13540965807159863, 0.1351585674504701, 0.13491125936798423, 0.13466767068891938, 0.13442773934419194, 0.13419140431283, 0.13395860560425255, 0.13372928424084793, 0.13350338224084699, 0.1332808426014858, 0.1330616092824533, 0.13284562718961865, 0.13263284215903354, 0.1324232009412056, 0.13221665118563727, 0.1320131414256262, 0.1318126210633228, 0.13161504035504046, 0.13142035039681396, 0.13122850311020234, 0.13103945122833172, 0.1308531482821742, 0.1306695485870585, 0.13048860722940933, 0.13031028005371023, 0.13013452364968742, 0.1299612953397103, 0.12979055316640456, 0.12962225588047543, 0.12945636292873622, 0.1292928344423398, 0.12913163122520863, 0.12897271474266087, 0.1288160471102284, 0.1286615910826643, 0.12850931004313595, 0.12835916799260091, 0.12821112953936273, 0.12806515988880282, 0.1279212248332866, 0.12777929074223984, 0.12763932455239288, 0.12750129375819014, 0.1273651664023615, 0.1272309110666531, 0.1270984968627151, 0.12696789342314324, 0.1268390708926723, 0.12671199991951793, 0.12658665164686533, 0.12646299770450173, 0.1263410102005903, 0.12622066171358307, 0.12610192528427094, 0.12598477440796768, 0.12586918302682656, 0.12575512552228646, 0.12564257670764595, 0.125531511820763, 0.12542190651687796, 0.12531373686155806, 0.12520697932376104, 0.12510161076901619, 0.12499760845272072, 0.12489495001354944, 0.12479361346697586, 0.12469357719890274, 0.12459481995940073, 0.12449732085655238, 0.12440105935040063, 0.12430601524699941, 0.12421216869256503, 0.12411950016772617, 0.12402799048187152, 0.1239376207675925, 0.12384837247522043, 0.12376022736745576, 0.12367316751408822, 0.12358717528680627, 0.12350223335409431, 0.12341832467621602, 0.12333543250028281, 0.12325354035540557, 0.1231726320479281, 0.1230926916567416, 0.1230137035286781, 0.12293565227398189, 0.12285852276185738, 0.12278230011609251, 0.12270696971075588, 0.122632517165967, 0.12255892834373741, 0.12248618934388288, 0.12241428650000435, 0.12234320637553679, 0.12227293575986503, 0.12220346166450505, 0.12213477131935015, 0.12206685216898036, 0.12199969186903432, 0.12193327828264255, 0.1218675994769208, 0.1218026437195232, 0.12173839947525317, 0.12167485540273175, 0.12161200035112266, 0.12154982335691221, 0.12148831364074386, 0.1214274606043065, 0.12136725382727503, 0.1213076830643028, 0.12124873824206511, 0.12119040945635222, 0.12113268696921199, 0.12107556120614067, 0.12101902275332105, 0.1209630623549076, 0.12090767091035698, 0.12085283947180453, 0.1207985592414843, 0.12074482156919317, 0.12069161794979762, 0.120638940020783, 0.1205867795598437, 0.1205351284825145, 0.12048397883984183, 0.12043332281609458, 0.12038315272651298, 0.12033346101509651, 0.12028424025242851, 0.12023548313353766, 0.12018718247579592, 0.12013933121685172, 0.12009192241259829, 0.12004494923517617, 0.11999840497100957, 0.11995228301887596, 0.1199065768880084, 0.11986128019622963, 0.11981638666811811, 0.11977189013320469, 0.1197277845242001, 0.11968406387525217, 0.11964072232023276, 0.11959775409105328, 0.11955515351600907, 0.11951291501815157, 0.11947103311368817, 0.11942950241040878, 0.11938831760613948, 0.11934747348722188, 0.11930696492701853, 0.11926678688444335, 0.11922693440251696, 0.11918740260694656, 0.11914818670472987, 0.11910928198278219, 0.11907068380658745, 0.11903238761887124, 0.118994388938297, 0.11895668335818378, 0.11891926654524589, 0.11888213423835378, 0.11884528224731584, 0.11880870645168097, 0.11877240279956114, 0.11873636730647416, 0.11870059605420569, 0.11866508518969063, 0.11862983092391369, 0.11859482953082796, 0.1185600773462924, 0.11852557076702673, 0.11849130624958448, 0.11845728030934304, 0.11842348951951098, 0.11838993051015206, 0.11835659996722582, 0.11832349463164421, 0.1182906112983446, 0.11825794681537785, 0.1182254980830122, 0.11819326205285235, 0.11816123572697322, 0.11812941615706837, 0.11809780044361308, 0.1180663857350411, 0.1180351692269359, 0.11800414816123508, 0.11797331982544865, 0.11794268155189007, 0.11791223071692081, 0.11788196474020712, 0.11785188108399001, 0.11782197725236687, 0.11779225079058586, 0.11776269928435175, 0.1177333203591439, 0.11770411167954535, 0.11767507094858358, 0.11764619590708232, 0.1176174843330243, 0.11758893404092469, 0.11756054288121534, 0.11753230873963921, 0.11750422953665518, 0.1174763032268528, 0.11744852779837713, 0.11742090127236303, 0.11739342170237937, 0.11736608717388218, 0.11733889580367743, 0.11731184573939273, 0.11728493515895806, 0.11725816227009504, 0.11723152530981512, 0.11720502254392601, 0.11717865226654672, 0.11715241279963057, 0.11712630249249624, 0.11710031972136711, 0.11707446288891774, 0.11704873042382877, 0.11702312078034874, 0.1169976324378636, 0.11697226390047354, 0.11694701369657676, 0.11692188037846035, 0.11689686252189838, 0.1168719587257561, 0.11684716761160184, 0.11682248782332426, 0.11679791802675718, 0.11677345690931004, 0.1167491031796049, 0.11672485556711944, 0.11670071282183621, 0.11667667371389756, 0.11665273703326642, 0.11662890158939308, 0.11660516621088733, 0.11658152974519619, 0.11655799105828717, 0.11653454903433691, 0.11651120257542477, 0.1164879506012322, 0.11646479204874653, 0.11644172587197021, 0.11641875104163471, 0.11639586654491951, 0.11637307138517557, 0.11635036458165367, 0.11632774516923725, 0.1163052121981797, 0.1162827647338463, 0.11626040185646037, 0.11623812266085354, 0.11621592625622078, 0.11619381176587887, 0.11617177832702948, 0.11614982509052611, 0.11612795122064482, 0.11610615589485901, 0.11608443830361793, 0.11606279765012895, 0.11604123315014338, 0.1160197440317464, 0.11599832953514966, 0.11597698891248827, 0.11595572142762073, 0.11593452635593232, 0.11591340298414188, 0.11589235061011186, 0.11587136854266147, 0.11585045610138303, 0.11582961261646167, 0.11580883742849751, 0.11578812988833147, 0.1157674893568736, 0.11574691520493456, 0.11572640681305964, 0.11570596357136612, 0.11568558487938266, 0.11566527014589202, 0.1156450187887761, 0.1156248302348638, 0.11560470391978125, 0.11558463928780467, 0.11556463579171584, 0.1155446928926596, 0.11552481006000434, 0.11550498677120447, 0.1154852225116651, 0.11546551677460946, 0.11544586906094816, 0.11542627887915065, 0.11540674574511939, 0.11538726918206528, 0.11536784872038602, 0.11534848389754587, 0.11532917425795812, 0.11530991935286866, 0.11529071874024246, 0.11527157198465128, 0.11525247865716359, 0.11523343833523622, 0.115214450602608, 0.11519551504919504, 0.11517663127098775, 0.11515779886994981, 0.11513901745391866, 0.11512028663650761, 0.11510160603701002, 0.1150829752803044, 0.11506439399676176, 0.11504586182215429, 0.11502737839756538, 0.11500894336930152, 0.11499055638880534, 0.11497221711257051, 0.11495392520205754, 0.11493568032361162, 0.11491748214838131, 0.11489933035223901, 0.11488122461570245, 0.1148631646238577, 0.11484515006628346, 0.11482718063697656, 0.11480925603427874, 0.11479137596080466, 0.11477354012337121, 0.11475574823292801, 0.11473800000448867, 0.11472029515706408, 0.11470263341359578, 0.11468501450089139, 0.11466743814956036, 0.1146499040939516, 0.11463241207209124, 0.11461496182562221, 0.11459755309974451, 0.11458018564315647, 0.11456285920799707, 0.1145455735497893, 0.11452832842738424, 0.11451112360290638, 0.11449395884169972, 0.1144768339122746, 0.1144597485862559, 0.11444270263833144, 0.11442569584620207, 0.11440872799053178, 0.11439179885489921, 0.11437490822574979, 0.11435805589234867, 0.11434124164673456, 0.11432446528367399, 0.11430772660061693, 0.11429102539765273, 0.11427436147746668, 0.11425773464529788, 0.1142411447088973, 0.11422459147848672, 0.11420807476671828, 0.114191594388635, 0.11417515016163157, 0.11415874190541604, 0.11414236944197212, 0.11412603259552212, 0.11410973119249034, 0.11409346506146748, 0.11407723403317513, 0.11406103794043132, 0.11404487661811634, 0.11402874990313934, 0.1140126576344053, 0.11399659965278268, 0.11398057580107171, 0.1139645859239729, 0.11394862986805648, 0.11393270748173197, 0.11391681861521855, 0.11390096312051583, 0.113885140851375, 0.1138693516632706, 0.1138535954133729, 0.11383787196052028, 0.11382218116519266, 0.11380652288948488, 0.11379089699708082, 0.11377530335322797, 0.11375974182471207, 0.11374421227983271, 0.11372871458837888, 0.11371324862160526, 0.1136978142522086, 0.11368241135430475, 0.11366703980340613, 0.11365169947639917, 0.11363639025152247, 0.1136211120083455, 0.11360586462774705, 0.1135906479918946, 0.11357546198422387, 0.11356030648941852, 0.11354518139339057, 0.1135300865832607, 0.11351502194733933, 0.11349998737510764, 0.11348498275719907, 0.11347000798538132, 0.11345506295253824, 0.1134401475526524, 0.11342526168078763, 0.11341040523307219, 0.11339557810668197, 0.11338078019982412, 0.11336601141172073, 0.11335127164259313, 0.11333656079364621, 0.11332187876705296, 0.1133072254659395, 0.11329260079437013, 0.11327800465733281, 0.11326343696072479, 0.11324889761133843, 0.11323438651684738, 0.11321990358579281, 0.11320544872757027, 0.11319102185241622, 0.11317662287139515, 0.11316225169638684, 0.1131479082400738, 0.11313359241592902, 0.11311930413820366, 0.11310504332191526, 0.113090809882836, 0.11307660373748117, 0.11306242480309785, 0.11304827299765373, 0.11303414823982605, 0.11302005044899109, 0.11300597954521326, 0.11299193544923496, 0.11297791808246613, 0.11296392736697435, 0.1129499632254747, 0.11293602558132021, 0.11292211435849227, 0.11290822948159113, 0.11289437087582664, 0.11288053846700934, 0.11286673218154104, 0.11285295194640659, 0.11283919768916484, 0.11282546933794016, 0.11281176682141433, 0.11279809006881791, 0.11278443900992242, 0.11277081357503228, 0.11275721369497715, 0.11274363930110379, 0.11273009032526908, 0.11271656669983204, 0.11270306835764694, 0.11268959523205577, 0.11267614725688131, 0.11266272436642018, 0.11264932649543599, 0.1126359535791525, 0.11262260555324709, 0.1126092823538443, 0.11259598391750933, 0.11258271018124176, 0.11256946108246932, 0.11255623655904196, 0.11254303654922566, 0.1125298609916966, 0.11251670982553554, 0.11250358299022184, 0.11249048042562801, 0.11247740207201426, 0.11246434787002296, 0.11245131776067332, 0.11243831168535622, 0.11242532958582899, 0.11241237140421047, 0.1123994370829757, 0.11238652656495157, 0.1123736397933113, 0.11236077671157034, 0.11234793726358125, 0.11233512139352943, 0.11232232904592837, 0.11230956016561533, 0.11229681469774694, 0.11228409258779495, 0.11227139378154191, 0.1122587182250772, 0.11224606586479272, 0.11223343664737893, 0.11222083051982111, 0.11220824742939514, 0.11219568732366401, 0.11218315015047374, 0.11217063585794984, 0.1121581443944938, 0.11214567570877926, 0.11213322974974854, 0.11212080646660945, 0.11210840580883147, 0.1120960277261427, 0.11208367216852645, 0.11207133908621798, 0.1120590284297014, 0.11204674014970636, 0.1120344741972053, 0.11202223052340994, 0.1120100090797687, 0.11199780981796353, 0.11198563268990717, 0.11197347764774021, 0.11196134464382819, 0.11194923363075919, 0.11193714456134077, 0.11192507738859742, 0.11191303206576808, 0.11190100854630332, 0.111889006783863, 0.11187702673231369, 0.11186506834572625, 0.11185313157837333, 0.11184121638472705, 0.11182932271945678, 0.11181745053742657, 0.1118055997936932, 0.11179377044350368, 0.11178196244229323, 0.11177017574568301, 0.11175841030947818, 0.11174666608966556, 0.11173494304241176, 0.11172324112406111, 0.11171156029113367, 0.11169990050032325, 0.11168826170849552, 0.11167664387268605, 0.11166504695009859, 0.111653470898103, 0.11164191567423376, 0.11163038123618789, 0.11161886754182328, 0.11160737454915722, 0.11159590221636424, 0.11158445050177491, 0.11157301936387391, 0.11156160876129849, 0.11155021865283694, 0.1115388489974269, 0.11152749975415395, 0.11151617088225006, 0.11150486234109198, 0.11149357409019998, 0.11148230608923627, 0.11147105829800355, 0.11145983067644369, 0.11144862318463639, 0.11143743578279766, 0.11142626843127865, 0.11141512109056421, 0.11140399372127169, 0.11139288628414973, 0.11138179874007667, 0.11137073105005976, 0.11135968317523372, 0.11134865507685937, 0.11133764671632285, 0.11132665805513417, 0.11131568905492617, 0.11130473967745334, 0.11129380988459076, 0.111282899638333, 0.11127200890079304, 0.11126113763420127, 0.11125028580090422, 0.11123945336336394, 0.11122864028415665, 0.11121784652597176, 0.11120707205161114, 0.111196316823988, 0.11118558080612581, 0.11117486396115757, 0.11116416625232485, 0.11115348764297682, 0.11114282809656935, 0.11113218757666424, 0.11112156604692819, 0.11111096347113213, 0.1111003798131502, 0.11108981503695904, 0.11107926910663696, 0.11106874198636303, 0.11105823364041653, 0.11104774403317584, 0.11103727312911793, 0.1110268208928175, 0.11101638728894624, 0.11100597228227219, 0.11099557583765872, 0.11098519792006427, 0.11097483849454122, 0.11096449752623545, 0.11095417498038561, 0.1109438708223224, 0.11093358501746786, 0.11092331753133491, 0.11091306832952645, 0.11090283737773492, 0.11089262464174166, 0.1108824300874161, 0.11087225368071549, 0.11086209538768385, 0.11085195517445186, 0.11084183300723595, 0.11083172885233784, 0.11082164267614396, 0.11081157444512484, 0.11080152412583467, 0.11079149168491069, 0.11078147708907268, 0.11077148030512234, 0.11076150129994296, 0.11075154004049863, 0.1107415964938341, 0.11073167062707387, 0.11072176240742206, 0.11071187180216167, 0.11070199877865425, 0.11069214330433941, 0.11068230534673422, 0.11067248487343292, 0.11066268185210644, 0.11065289625050183, 0.11064312803644193, 0.11063337717782494, 0.11062364364262389, 0.11061392739888637, 0.11060422841473402, 0.11059454665836199, 0.11058488209803878, 0.1105752347021058, 0.11056560443897669, 0.11055599127713732, 0.11054639518514517, 0.11053681613162895, 0.11052725408528831, 0.11051770901489343, 0.1105081808892847, 0.11049866967737226, 0.11048917534813572, 0.11047969787062377, 0.11047023721395391, 0.1104607933473119, 0.11045136623995175, 0.11044195586119507, 0.11043256218043096, 0.11042318516711544, 0.11041382479077144, 0.11040448102098831, 0.11039515382742138, 0.11038584317979189, 0.11037654904788657, 0.11036727140155737, 0.11035801021072106, 0.11034876544535903, 0.11033953707551701, 0.11033032507130472, 0.11032112940289565, 0.11031195004052669, 0.11030278695449791, 0.11029364011517226, 0.11028450949297533, 0.11027539505839512, 0.1102662967819816, 0.11025721463434669, 0.11024814858616373, 0.11023909860816751, 0.11023006467115376, 0.11022104674597905, 0.11021204480356056, 0.11020305881487565, 0.11019408875096186, 0.11018513458291654, 0.11017619628189651, 0.1101672738191181, 0.11015836716585667, 0.11014947629344649, 0.11014060117328041, 0.11013174177680991, 0.11012289807554448, 0.1101140700410517, 0.11010525764495695, 0.11009646085894312, 0.11008767965475043, 0.11007891400417633, 0.11007016387907508, 0.11006142925135773, 0.11005271009299186, 0.1100440063760013, 0.11003531807246607, 0.11002664515452207, 0.11001798759436081, 0.11000934536422959, 0.11000071843643078, 0.10999210678332201, 0.10998351037731588, 0.10997492919087967, 0.10996636319653533, 0.1099578123668592, 0.10994927667448172, 0.10994075609208753, 0.10993225059241497, 0.10992376014825629, 0.10991528473245701, 0.10990682431791607, 0.10989837887758558, 0.10988994838447066, 0.10988153281162927, 0.10987313213217194, 0.10986474631926178, 0.10985637534611416, 0.10984801918599674, 0.10983967781222898, 0.10983135119818241, 0.1098230393172801, 0.10981474214299666, 0.10980645964885817, 0.10979819180844184, 0.10978993859537595, 0.10978169998333977, 0.10977347594606332, 0.10976526645732715, 0.10975707149096235, 0.10974889102085035, 0.10974072502092272, 0.10973257346516105, 0.10972443632759692, 0.10971631358231154, 0.10970820520343581, 0.10970011116515, 0.10969203144168382, 0.10968396600731617, 0.10967591483637493, 0.10966787790323693, 0.1096598551823278, 0.10965184664812183, 0.10964385227514184, 0.10963587203795896, 0.10962790591119273, 0.10961995386951068, 0.10961201588762844, 0.10960409194030939, 0.10959618200236484, 0.10958828604865356, 0.10958040405408187, 0.10957253599360356, 0.10956468184221951, 0.10955684157497782, 0.1095490151669736, 0.10954120259334879, 0.10953340382929216, 0.10952561885003913, 0.10951784763087159, 0.10951009014711792, 0.10950234637415272, 0.1094946162873968, 0.10948689986231708, 0.10947919707442638, 0.10947150789928335, 0.10946383231249239, 0.10945617028970353, 0.10944852180661227, 0.1094408868389595, 0.10943326536253145, 0.10942565735315939, 0.10941806278671978, 0.109410481639134, 0.10940291388636826, 0.10939535950443353, 0.10938781846938536, 0.10938029075732388, 0.10937277634439364, 0.10936527520678356, 0.10935778732072658, 0.1093503126625, 0.10934285120842502, 0.10933540293486667, 0.10932796781823395, 0.10932054583497941, 0.10931313696159928, 0.10930574117463332, 0.10929835845066466, 0.10929098876631971, 0.10928363209826811, 0.10927628842322265, 0.10926895771793908, 0.10926163995921614, 0.10925433512389526, 0.10924704318886075, 0.10923976413103943, 0.10923249792740072, 0.10922524455495645, 0.1092180039907608, 0.10921077621191025, 0.10920356119554332, 0.1091963589188408, 0.10918916935902521, 0.10918199249336116, 0.10917482829915494, 0.1091676767537546, 0.10916053783454976, 0.10915341151897164, 0.10914629778449267, 0.10913919660862695, 0.1091321079689296, 0.10912503184299699, 0.10911796820846656, 0.10911091704301669, 0.10910387832436674, 0.1090968520302769, 0.109089838138548, 0.10908283662702158, 0.10907584747357968, 0.10906887065614493, 0.1090619061526802, 0.10905495394118876, 0.10904801399971407, 0.10904108630633974, 0.10903417083918945, 0.1090272675764268, 0.10902037649625529, 0.10901349757691824, 0.10900663079669869, 0.10899977613391931, 0.10899293356694231, 0.10898610307416945, 0.10897928463404175, 0.10897247822503968, 0.1089656838256829, 0.10895890141453021, 0.10895213097017944, 0.10894537247126751, 0.10893862589647023, 0.10893189122450218, 0.10892516843411676, 0.10891845750410602, 0.1089117584133007, 0.10890507114056985, 0.10889839566482128, 0.10889173196500086, 0.10888508002009295, 0.10887843980912004, 0.1088718113111428, 0.10886519450525992, 0.10885858937060816, 0.10885199588636209, 0.10884541403173417, 0.10883884378597458, 0.10883228512837129, 0.10882573803824976, 0.10881920249497302, 0.10881267847794161, 0.10880616596659344, 0.10879966494040369, 0.10879317537888483, 0.10878669726158648, 0.10878023056809534, 0.10877377527803517, 0.10876733137106667, 0.1087608988268874, 0.10875447762523173, 0.10874806774587079, 0.10874166916861233, 0.10873528187330074, 0.10872890583981681, 0.10872254104807796, 0.10871618747803793, 0.10870984510968663]\n"
     ]
    }
   ],
   "source": [
    "# Generate random test data\n",
    "np.random.seed(0) # For reproducibility\n",
    "X = np.random.rand(100, 3) # 100 samples, 3 features\n",
    "Y = np.random.rand(100)\n",
    "W = np.random.rand(3) # Initial guess for parameters\n",
    "# Set hyperparameters\n",
    "alpha = 0.01\n",
    "iterations = 1000\n",
    "# Test the gradient_descent function\n",
    "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
    "# Print the final parameters and cost history\n",
    "print(\"Final Parameters:\", final_params)\n",
    "print(\"Cost History:\", cost_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation - RMSE\n",
    "def rmse(Y, Y_pred):\n",
    "    \"\"\"\n",
    "    This Function calculates the Root Mean Squres.\n",
    "    Input Arguments:\n",
    "      Y: Array of actual(Target) Dependent Varaibles.\n",
    "      Y_pred: Array of predeicted Dependent Varaibles.\n",
    "    Output Arguments:\n",
    "      rmse: Root Mean Square.\n",
    "    \"\"\"\n",
    "    errors = Y - Y_pred\n",
    "    squared_errors = errors**2\n",
    "    mse = np.sum(squared_errors) / len(Y)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation - R2\n",
    "def r2(Y, Y_pred):\n",
    "    \"\"\"\n",
    "    This Function calculates the R Squared Error.\n",
    "    Input Arguments:\n",
    "      Y: Array of actual(Target) Dependent Varaibles.\n",
    "      Y_pred: Array of predeicted Dependent Varaibles.\n",
    "    Output Arguments:\n",
    "      rsquared: R Squared Error.\n",
    "    \"\"\"\n",
    "    mean_y = np.mean(Y)\n",
    "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
    "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Weights: [0.34873377 0.65071188]\n",
      "Cost History (First 10 iterations): [4016.884594476226, 3284.980944892118, 2687.5579505822257, 2199.9056192878434, 1801.8542679810128, 1476.940268944027, 1211.7250848802507, 995.239427138732, 818.5295464493356, 674.2868899472107]\n",
      "RMSE on Test Set: 5.524176336610737\n",
      "R-Squared on Test Set: 0.8733843365266284\n"
     ]
    }
   ],
   "source": [
    "# Main Function\n",
    "def main():\n",
    "  # Step 1: Load the dataset\n",
    "  data = pd.read_csv('../datasets/student.csv')\n",
    "\n",
    "  # Step 2: Split the data into features (X) and target (Y)\n",
    "  X = data[['math score', 'reading score']].values # Features: Math and Reading marks\n",
    "  Y = data['writing score'].values # Target: Writing marks\n",
    "\n",
    "  # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
    "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_seed=42)\n",
    "\n",
    "  # Step 4: Initialize weights (W) to zeros, learning rate and number of iterations\n",
    "  W = np.zeros(X_train.shape[1]) # Initialize weights\n",
    "  alpha = 0.00001 # Learning rate\n",
    "  iterations = 1000 # Number of iterations for gradient descent\n",
    "\n",
    "  # Step 5: Perform Gradient Descent\n",
    "  W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
    "\n",
    "  # Step 6: Make predictions on the test set\n",
    "  Y_pred = np.dot(X_test, W_optimal)\n",
    "\n",
    "  # Step 7: Evaluate the model using RMSE and R-Squared\n",
    "  model_rmse = rmse(Y_test, Y_pred)\n",
    "  model_r2 = r2(Y_test, Y_pred)\n",
    "\n",
    "  # Step 8: Output the results\n",
    "  print(\"Final Weights:\", W_optimal)\n",
    "  print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
    "  print(\"RMSE on Test Set:\", model_rmse)\n",
    "  print(\"R-Squared on Test Set:\", model_r2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
